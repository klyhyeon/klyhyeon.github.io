---
layout: post
title: 대규모 트랜잭션을 처리하는 배민 주문시스템 규모에 따른 진화
image: 
  path: /assets/img/blog/new_years_card_297_2023_a.jpg
#description: >
#  Version 9 is the most complete version of Hydejack yet.
#  Modernized design, big headlines, and big new features.
sitemap: false
---

대규모 트랜잭션을 처리하는 배민 주민시스템 규모에 따른 진화 - 강홍구(푸드주문서버개발팀)
{:.lead}

5년 전 배달의민족에 합류했을 때, 주문시스템은 급격하게 증가하는 주문수를 처리하기 어려운 시스템이었습니다.
5년간 주문시스템은 '주문DB 샤딩', 'CQRS 적용' 등 여러 작업들을 통해 일 300만 건 이상의 주문수를 처리하는 안정적인 시스템이 되었습니다.
대규모 트랜잭션을 처리하는 배민 주문시스템이 변화해간 내용을 공유합니다.

- Table of Contents
{:toc .large-only}

## 배달의 민족 주문 시스템

장바구니 - 주문하기 - 주문내역
점심, 저녁시간 트래픽 급증

MSA 마려움
- 가게, 메뉴, 주문, 결제, 배달 시스템들이 통신
- 다른 시스템에 문제가 생겨도 주문이 가능해야 함

대용량 데이터
- 일 평균 300만건 저장, 수년간의 데이터 보관, 관리

대규모 트랜잭션
- 점심, 저녁시간 몰리는 트랜잭션 대응

여러 시스템과 여계
- 이벤트 기반(유실 됐을 때 대응)
- MSA 이벤트 일관성

## 성장하는 배민 주문 시스템

일 주문건수 2018(약 30만) - 2023(약 350만)
- 단일 장애 포인트가 전체 시스템의 장애로 이어짐
- 대용량 데이터 - RDBMS 조인 연산으로 조회 성능 좋지 않음
- 대규모 트랜잭션 - 주문수 증가로 저장소의 분당 쓰기 처리량 한계에 도달
- 복잡한 이벤트 아키텍처 - 규칙 없는 이벤트 발행으로 유실되는 이벤트 많아지고 서비스 복잡도 높아짐

## #단일 장애 포인트

MSA 적용(MQ 이용)

중앙 집중 DB의 장애가 전체 시스템으로 변화
주문 시스템 -> **RUBY(중앙집중형 장비 DB)** -> 주문 중계, 가게 시스템, ETC 시스템
각 시스템 (주문 시스템) (가게 시스템) (ETC SyStem)을 별도의 DB를 가지게 해 탈 루비 성공. MQ로 이벤트 기반 아키텍처 사용
MQ 지연돼도 MQ 재소비가 가능하기에 지연을 줄임

## 대용량 데이터

주문 요청 (command: 주문 API) -> 주문 이벤트 발생 -> 주문MQ // 조회 요청(query: 주문 인터널 API)
주문내역 상세에 정말 많은 데이터가 필요함

주문 애그리거트 기술로 조인 연산이 필요했었음 -> 조인, 조인, line item, delivery, seller_sumary...
mongoDB를 이용해 애그리거트 역정규화로 테이블을 하나의 도큐먼트로 구성함

데이터 동기화
- 주문 이벤트 발생 -> 처리기 -> 몽고 DB에 주문 데이터 동기화 -> 주문 조회 요청(주문 인터널 API)도 몽고 DB를 조회

결론: 커맨드 모델과 조회 모델을 분리하고 조회 모델 역 정규화를 통해 해결

## 대규모 트랜잭션

CQRS 

아래와 같은 구조 HA(High availability?) 구성
주문 API -> (쓰기 요청) 주문 DB(primary) writer
주문 인터널 API(실시간 조회) -> reader 주문 DB(replica)

하지만 스펙업 때문에 

해결책은 샤딩
샤드 클러스터를 구성해 쓰기 부하를 분한
하지만 AWS 오로라는 샤딩을 지원하지 않음

코드 단에서 애플리케이션 샤딩을 구현하자

- 데이터 저장 시 어떤 샤드에 접근할지 결정하는 샤딩전략 고민
- 여러 샤드에 있는 데이터를 애그리게이트 하는 방법에 대한 고민

샤딩 전략
- key Based
    - hash based 주문번호를 Hash function을 돌려서 나온 해시 값으로 샤드에 이어줌(주문 A - 해시값 1 - 샤드 1)
    - 구현 간단, 샤드 클러스터 , 데이터 동적으로 재배치할 때 샤드 변경
- Range based sharding
  - 주문 가격 기반으로 샤딩함
  - 특정 값의 범위 기반(5천원~1만원)으로 샤드를 결정하면 돼서 구현이 간단
  - 데이터가 균등하게 배분되지 않아 특정 샤드에 부하가 될 수 있음
- Directory Based
  - Key based와 유사한데 룩업 테이블이 있음
  - 동적으로 샤드를 추가하는데 유리
  - 룩업 테이블이 단일 장애 포인트가 될 수 있음

주문 시스템의 특징
- 주문이 정상 동작하지 않으면 배민 전체 서비스의 좋지 않은 경험이 된다 -> 단일 장애 포인트는 피함
- `동적` 주문 데이터는 최대 30일 만 저장한다. -> 샤드 추가 이후 30일이 지나면 데이터는 다시 균등하게 분배된다.
- 결정은 Key Based 샤딩
주문 순번 % 샤드 수 = 샤드번호
- 주문번호 샤드 키를 사용해 균등 분배

(_애플리케이션 예시 코드 사진있음_)

다건 주문 내역 조회도 샤딩과 몽고 DB로 해소

## 복잡한 이벤트 아키텍처

이벤트를 기반으로 관심사를 분리

주문 생성(OPENDED) ACCEPTED, CLOSED, CANCELED

OrderDomainService -> spring event -> OrderOpen(Accept, close, cancelled)PostProcessService -> sqs event

문제점 1
주문 API - 주문 생성 알림 전송 요청
주문배치 - - 주문 취소 알림 전송 요청, 현금 영수증 -> 알람전송
이벤트 처리기  --현금 영수정 -> 현금 영수정

문제점 2
이벤트 유실 발생했을 때 재처리가 어려웠음
주문 도메인 이벤트는 내부 이벤트로 정의, 서비스 로직은 외부 이벤트로 정의

주문 API 여러개 -> SQS(orderNO, orderStatus) -> 이벤트 처리기(주문 저장소: 현금영수증이나 주문 생성에 필요한 정보들을 조회해서 채워줌) (mongoDB NoSQL을 사용하기 때문에 부담없었음)


이벤트 처리 주체의 단일화

도메인은 도메인 이벤트만 발행, 이벤트 스냅샷 (SQS) -> 이벤트 

이벤트 발행 실패 유형
- 트랜잭션 안에서 이벤트 발생 실패: 도메인 로직 전체가 실패(이슈 없음)
- 도메인 로직은 성공 이벤트 발생 실패로 서비스 로직 재실행 못함
트랜잭션 아웃박스 패턴
- 이벤트 발생 실패와 서비스 실패를 격리하여 재발행 수단을 보장
서비스 로직 -> 아웃박스 -> 이벤트 publish
- 이벤트 유실이 발생해서 이벤트 퍼블리시가 아웃박스를 통해 payload를 재소비


## 요약

- MQ(MSA)
- 샤드 클러스터(대용량 데이터)
- CQRS(대규모 트랜잭션)